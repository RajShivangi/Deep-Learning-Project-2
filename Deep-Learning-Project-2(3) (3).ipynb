{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed89725b-89b9-4412-91da-4238acaeb6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /opt/anaconda3/lib/python3.12/site-packages (1.7.4.2)\n",
      "Requirement already satisfied: bleach in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (4.1.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (2.0.4)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (3.7)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (3.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (2.32.2)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (69.5.1)\n",
      "Requirement already satisfied: six>=1.10 in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: text-unidecode in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (4.66.4)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (2.2.2)\n",
      "Requirement already satisfied: webencodings in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (0.5.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from bleach->kaggle) (23.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fc2f144-fbba-4cff-8153-320ba421f406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-------@ 1 sanyuktatuti  staff  70 Apr  8 17:01 /Users/sanyuktatuti/.kaggle/kaggle.json\n"
     ]
    }
   ],
   "source": [
    "!ls -l ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c143dd2-51af-41a7-bd2f-99f3c56ca7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions download -c deep-learning-spring-2025-project-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "956771d3-c3c3-4bc1-bfcc-3767bbd90073",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -oq deep-learning-spring-2025-project-2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b144556d-6646-4f25-a54d-61bc9d1a96ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in current directory: ['deep-learning-spring-2025-project-2.zip', 'Deep-Learning-Project-2(2).ipynb', 'results', 'Deep-Learning-Project-2(1).ipynb', 'test_unlabelled.pkl', 'Deep-Learning-Project-2(3).ipynb', '.ipynb_checkpoints']\n",
      "test_unlabelled.pkl found!\n"
     ]
    }
   ],
   "source": [
    "# List the files in the current directory to verify the data is extracted\n",
    "import os\n",
    "print(\"Files in current directory:\", os.listdir())\n",
    "\n",
    "# Check for the existence of test_unlabelled.pkl\n",
    "if os.path.exists(\"test_unlabelled.pkl\"):\n",
    "    print(\"test_unlabelled.pkl found!\")\n",
    "else:\n",
    "    print(\"test_unlabelled.pkl not found. Please check if the data has been downloaded and unzipped correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "234edb8e-4815-47bc-bb53-8aa0da2ea99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.46.3)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.5.0)\n",
      "Requirement already satisfied: evaluate in /opt/anaconda3/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: peft in /opt/anaconda3/lib/python3.12/site-packages (0.15.1)\n",
      "Requirement already satisfied: trl in /opt/anaconda3/lib/python3.12/site-packages (0.16.1)\n",
      "Requirement already satisfied: bitsandbytes in /opt/anaconda3/lib/python3.12/site-packages (0.42.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from trl) (13.3.5)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->trl) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->trl) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->trl) (0.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: nvidia-ml-py3 in /opt/anaconda3/lib/python3.12/site-packages (7.352.0)\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# 1. Install Required Libraries\n",
    "# =====================================\n",
    "!pip install transformers datasets evaluate accelerate peft trl bitsandbytes\n",
    "!pip install nvidia-ml-py3\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    RobertaTokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorWithPadding, \n",
    "    RobertaForSequenceClassification,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset, Dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afdf6e55-b208-4502-9799-e25601aeb775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# 2. Set Device to Use MPS if Available\n",
    "# =====================================\n",
    "def get_device():\n",
    "    # Prefer MPS for macOS, otherwise use CUDA if available; else use CPU.\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae15a123-5aa3-4ab9-afc1-7cfe92524611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 4\n",
      "Class names: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# 3. Load the AG News Dataset and Preprocess\n",
    "# =====================================\n",
    "base_model = 'roberta-base'\n",
    "dataset = load_dataset('ag_news', split='train')\n",
    "tokenizer = RobertaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "num_labels = dataset.features['label'].num_classes\n",
    "class_names = dataset.features[\"label\"].names\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(class_names)}\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "becb9b9f-1e40-4521-b233-ab8e48652eef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: ['text', 'label']\n",
      "\n",
      "Dataset features:\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['World', 'Sports', 'Business', 'Sci/Tech'], id=None)}\n",
      "\n",
      "First 5 examples:\n",
      "Example 0: {'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n",
      "Example 1: {'text': 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.', 'label': 2}\n",
      "Example 2: {'text': \"Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums.\", 'label': 2}\n",
      "Example 3: {'text': 'Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\\\flows from the main pipeline in southern Iraq after\\\\intelligence showed a rebel militia could strike\\\\infrastructure, an oil official said on Saturday.', 'label': 2}\n",
      "Example 4: {'text': 'Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.', 'label': 2}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the AG News dataset (the training split for inspection)\n",
    "dataset = load_dataset('ag_news', split='train')\n",
    "\n",
    "# Print the column names\n",
    "print(\"Column names:\", dataset.column_names)\n",
    "\n",
    "# Print the dataset features (schema)\n",
    "print(\"\\nDataset features:\")\n",
    "print(dataset.features)\n",
    "\n",
    "# Show a few examples from the dataset\n",
    "print(\"\\nFirst 5 examples:\")\n",
    "for i in range(5):\n",
    "    print(f\"Example {i}: {dataset[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ead9d68-9b46-4be9-b3ae-9392cde360cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded and moved to device.\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "\n",
      "PEFT Model with LoRA created. Trainable parameters:\n",
      "trainable params: 999,172 || all params: 125,647,880 || trainable%: 0.7952\n",
      "Total trainable parameters: 999172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# 4. Load Pre-trained RoBERTa Model & Apply LoRA\n",
    "# =====================================\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    id2label=id2label,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "model.to(device)\n",
    "print(\"Base model loaded and moved to device.\")\n",
    "\n",
    "# Set up LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=11,                    \n",
    "    lora_alpha=8,           \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"query\", \"key\"],  \n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.to(device)\n",
    "print(\"\\nPEFT Model with LoRA created. Trainable parameters:\")\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ddaa480-6da4-4a79-b723-a1959121a01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 119360\n",
      "Eval dataset size: 640\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# 5. Create Train and Evaluation Splits\n",
    "# =====================================\n",
    "split_datasets = tokenized_dataset.train_test_split(test_size=640, seed=42)\n",
    "train_dataset = split_datasets['train']\n",
    "eval_dataset = split_datasets['test']\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Eval dataset size: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01348eb6-de89-472b-8711-62c073968c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7800' max='14920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7800/14920 1:25:19 < 1:17:54, 1.52 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.666100</td>\n",
       "      <td>0.373921</td>\n",
       "      <td>0.895312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.352800</td>\n",
       "      <td>0.357309</td>\n",
       "      <td>0.904687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.349700</td>\n",
       "      <td>0.308506</td>\n",
       "      <td>0.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.335997</td>\n",
       "      <td>0.893750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.304400</td>\n",
       "      <td>0.304992</td>\n",
       "      <td>0.903125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.279200</td>\n",
       "      <td>0.300651</td>\n",
       "      <td>0.901563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.281100</td>\n",
       "      <td>0.277787</td>\n",
       "      <td>0.903125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.294800</td>\n",
       "      <td>0.268388</td>\n",
       "      <td>0.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.281000</td>\n",
       "      <td>0.265158</td>\n",
       "      <td>0.917188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.264700</td>\n",
       "      <td>0.269578</td>\n",
       "      <td>0.910937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.281500</td>\n",
       "      <td>0.265299</td>\n",
       "      <td>0.917188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.265905</td>\n",
       "      <td>0.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.298900</td>\n",
       "      <td>0.242322</td>\n",
       "      <td>0.917188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.236500</td>\n",
       "      <td>0.259769</td>\n",
       "      <td>0.909375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.268200</td>\n",
       "      <td>0.248674</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.282400</td>\n",
       "      <td>0.257230</td>\n",
       "      <td>0.918750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.243400</td>\n",
       "      <td>0.250647</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.263100</td>\n",
       "      <td>0.273008</td>\n",
       "      <td>0.910937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.247400</td>\n",
       "      <td>0.263284</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.274100</td>\n",
       "      <td>0.240605</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.245300</td>\n",
       "      <td>0.249730</td>\n",
       "      <td>0.915625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.256700</td>\n",
       "      <td>0.228450</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.219300</td>\n",
       "      <td>0.223374</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.255900</td>\n",
       "      <td>0.241265</td>\n",
       "      <td>0.918750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.235500</td>\n",
       "      <td>0.227879</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.258500</td>\n",
       "      <td>0.229013</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.219700</td>\n",
       "      <td>0.221174</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.239500</td>\n",
       "      <td>0.205448</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.239100</td>\n",
       "      <td>0.241074</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.237600</td>\n",
       "      <td>0.235352</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.259300</td>\n",
       "      <td>0.218828</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.253739</td>\n",
       "      <td>0.917188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.208200</td>\n",
       "      <td>0.260526</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.220700</td>\n",
       "      <td>0.210960</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.228800</td>\n",
       "      <td>0.239019</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>0.242541</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.220200</td>\n",
       "      <td>0.215787</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.206900</td>\n",
       "      <td>0.237939</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.230100</td>\n",
       "      <td>0.211102</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.241700</td>\n",
       "      <td>0.216696</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.221400</td>\n",
       "      <td>0.238444</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.217100</td>\n",
       "      <td>0.225312</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.205175</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.229200</td>\n",
       "      <td>0.219841</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>0.233514</td>\n",
       "      <td>0.923438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.235000</td>\n",
       "      <td>0.234909</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.203800</td>\n",
       "      <td>0.221756</td>\n",
       "      <td>0.923438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.235500</td>\n",
       "      <td>0.264933</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.242700</td>\n",
       "      <td>0.213733</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.215600</td>\n",
       "      <td>0.232763</td>\n",
       "      <td>0.923438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.205200</td>\n",
       "      <td>0.218626</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.222100</td>\n",
       "      <td>0.224742</td>\n",
       "      <td>0.923438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.158900</td>\n",
       "      <td>0.231912</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.176700</td>\n",
       "      <td>0.233056</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.199200</td>\n",
       "      <td>0.226017</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.216186</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.222500</td>\n",
       "      <td>0.214830</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.230700</td>\n",
       "      <td>0.227312</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.220400</td>\n",
       "      <td>0.206239</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>0.201197</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.238103</td>\n",
       "      <td>0.918750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.219500</td>\n",
       "      <td>0.212930</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.210100</td>\n",
       "      <td>0.215611</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.215300</td>\n",
       "      <td>0.215795</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.219800</td>\n",
       "      <td>0.207610</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.226916</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.215600</td>\n",
       "      <td>0.208231</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.234300</td>\n",
       "      <td>0.197057</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.183100</td>\n",
       "      <td>0.225209</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.217100</td>\n",
       "      <td>0.213522</td>\n",
       "      <td>0.934375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.205700</td>\n",
       "      <td>0.212703</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.212200</td>\n",
       "      <td>0.205129</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.193100</td>\n",
       "      <td>0.249775</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.232300</td>\n",
       "      <td>0.220460</td>\n",
       "      <td>0.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.232200</td>\n",
       "      <td>0.208673</td>\n",
       "      <td>0.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.222500</td>\n",
       "      <td>0.222829</td>\n",
       "      <td>0.923438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.198200</td>\n",
       "      <td>0.214192</td>\n",
       "      <td>0.932813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.210300</td>\n",
       "      <td>0.206062</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# 6. Training Setup with Trainer\n",
    "# =====================================\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "output_dir = \"results\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    report_to=None,\n",
    "    eval_strategy='steps',\n",
    "    logging_steps=100,\n",
    "    learning_rate=9e-4,\n",
    "    num_train_epochs=2,\n",
    "    #max_steps=1200,\n",
    "    load_best_model_at_end=True,            \n",
    "    metric_for_best_model=\"eval_loss\",        \n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    optim=\"adamw_hf\",\n",
    "    gradient_checkpointing=False,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': True}\n",
    ")\n",
    "\n",
    "def get_trainer(model):\n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]  \n",
    "    )\n",
    "\n",
    "trainer = get_trainer(peft_model)\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "train_result = trainer.train()\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "885daa26-2427-489a-bcc4-dd600f431ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics: {'eval_loss': 0.20119670033454895, 'eval_accuracy': 0.9375, 'eval_runtime': 9.1448, 'eval_samples_per_second': 69.985, 'eval_steps_per_second': 1.094, 'epoch': 1.0455764075067024}\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# 7. Evaluate the Model\n",
    "# =====================================\n",
    "print(\"\\nEvaluating on validation set...\")\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(f\"Evaluation metrics: {eval_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6e2d0da-d3c5-46df-8c95-3369e8cf4e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 80/80 [00:11<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running inference on test_unlabelled.pkl...\n",
      "Type of unlabelled_obj: <class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a079ba909b4b3082d64f6eabb71253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [01:13<00:00, 13.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete. Predictions saved to results/inference_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# 8. Inference on Unlabeled Test Data\n",
    "# =====================================\n",
    "def evaluate_model(inference_model, dataset, labelled=True, batch_size=8, data_collator=None):\n",
    "    from torch.utils.data import DataLoader\n",
    "    eval_dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator)\n",
    "    current_device = get_device()\n",
    "    inference_model.to(current_device)\n",
    "    inference_model.eval()\n",
    "    all_predictions = []\n",
    "    if labelled:\n",
    "        metric = evaluate.load('accuracy')\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        batch = {k: v.to(current_device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = inference_model(**batch)\n",
    "        preds = outputs.logits.argmax(dim=-1).cpu()\n",
    "        all_predictions.append(preds)\n",
    "        if labelled:\n",
    "            references = batch[\"labels\"].cpu()\n",
    "            metric.add_batch(\n",
    "                predictions=preds.numpy(),\n",
    "                references=references.numpy()\n",
    "            )\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    if labelled:\n",
    "        eval_metric = metric.compute()\n",
    "        return eval_metric, all_predictions\n",
    "    return all_predictions\n",
    "\n",
    "_, _ = evaluate_model(peft_model, eval_dataset, labelled=True, batch_size=8, data_collator=data_collator)\n",
    "\n",
    "# -------------------------------------\n",
    "# Inference on Unlabelled Test Data\n",
    "# -------------------------------------\n",
    "print(\"\\nRunning inference on test_unlabelled.pkl...\")\n",
    "unlabelled_obj = pd.read_pickle(\"test_unlabelled.pkl\")\n",
    "print(\"Type of unlabelled_obj:\", type(unlabelled_obj))\n",
    "\n",
    "if isinstance(unlabelled_obj, pd.DataFrame):\n",
    "    unlabelled_dataset = Dataset.from_pandas(unlabelled_obj)\n",
    "else:\n",
    "    unlabelled_dataset = unlabelled_obj\n",
    "\n",
    "# Apply tokenization preprocessing\n",
    "test_dataset = unlabelled_dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Run inference (without expecting labels)\n",
    "preds = evaluate_model(peft_model, test_dataset, labelled=False, batch_size=8, data_collator=data_collator)\n",
    "\n",
    "df_output = pd.DataFrame({\n",
    "    'ID': range(len(preds)),\n",
    "    'Label': preds.numpy()  \n",
    "})\n",
    "\n",
    "submission_file = os.path.join(output_dir, \"inference_output.csv\")\n",
    "df_output.to_csv(submission_file, index=False)\n",
    "print(f\"Inference complete. Predictions saved to {submission_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d693254-8d12-4fcd-ab6c-49f5f33bbf42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
